# -*- coding: utf-8 -*-
"""Big-Mart_Sales_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1latr1E6XMYkLHo2kPXuzJk5IXSSg8aMr

#Import Libraries
"""

import numpy as np
import math
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV

"""# Read the Dataset"""

# loading the data to Pandas DataFrame
df_data = pd.read_csv('/content/train.csv')

# Showing the first five rows
df_data.head()

# Number of data points and features
df_data.shape

# Infomation about the data
df_data.info()

"""##Categorical Features:

- Item_Identifier
- Item_Fat_Content
- Item_Type
- Outlet_Identifier
- Outlet_Size
- Outlet_Location_Type
- Outlet_Type

##Numerical Features:

- Item_Weight
- Item_Visibility
- Item_MRP
- Outlet_Establishment_Year
- Item_Outlet_Sales
"""

# Checking for missing values
df_data.isnull().sum()

"""Replace missing values:

- Numerical feature --> Mean
- Categorical feature --> Mode
"""

# Mean of "Item_Weight" feature
df_data['Item_Weight'].mean()

# filling the missing values in "Item_weight" column with Mean
df_data['Item_Weight'].fillna(df_data['Item_Weight'].mean(), inplace=True)

df_data.isnull().sum()

# mode of "Outlet_Size" column
df_data['Outlet_Size'].mode()

# filling the missing values in "Outlet_Size" column with Mode
df_data['Outlet_Size'].fillna(df_data['Outlet_Size'].mode()[0],inplace=True)

df_data.isnull().sum()

df_data['Item_Fat_Content'].value_counts()

df_data.replace({'Item_Fat_Content': {'low fat':'Low Fat','LF':'Low Fat', 'reg':'Regular'}}, inplace=True)

df_data['Item_Fat_Content'].value_counts()

"""# Exploratory Data Analysis

### EDA using pandas profiling
"""

!pip install pandas_profiling

import pandas_profiling as pp

profile = pp.ProfileReport(df_data, title ="Pandas Profiling Report")

profile

plt.figure(figsize=(10,5))
sns.heatmap(df_data.corr(),annot=True)
plt.show()

"""## EDA using Klib library"""

!pip install klib

import klib

klib.cat_plot(df_data) # returns a visualization of the number and frequency of categorical features

klib.dist_plot(df_data) # returns a distribution plot for every numeric feature

klib.data_cleaning(df_data) # performs datacleaning (drop duplicates & empty rows/cols, adjust dtypes,...)

klib.clean_column_names(df_data) # cleans and standardizes column names, also called inside data_cleaning()

df_data.info()

"""#Pre-processing

##Label Encoding
"""

encoder = LabelEncoder()

df_data['item_identifier'] = encoder.fit_transform(df_data['item_identifier'])

df_data['item_fat_content'] = encoder.fit_transform(df_data['item_fat_content'])

df_data['item_type'] = encoder.fit_transform(df_data['item_type'])

df_data['outlet_identifier'] = encoder.fit_transform(df_data['outlet_identifier'])

df_data['outlet_size'] = encoder.fit_transform(df_data['outlet_size'])

df_data['outlet_location_type'] = encoder.fit_transform(df_data['outlet_location_type'])

df_data['outlet_type'] = encoder.fit_transform(df_data['outlet_type'])

df_data.head()

"""##Splitting Features and Target"""

X = df_data.drop(columns='item_outlet_sales', axis=1)
Y = df_data['item_outlet_sales']

print(X)

print(Y)

"""##Splitting dataset into Training and Testing set"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

"""#Model Implementation

##Linear Regression
"""

from sklearn.linear_model import LinearRegression
lr= LinearRegression()

# Train the model on training set

lr.fit(X_train,Y_train)

training_data_prediction_lr = lr.predict(X_train)

# linear regression model performance on training set

# R squared error
r2_train_lr = metrics.r2_score(Y_train, training_data_prediction_lr)
print('R Squared value = ', r2_train_lr)

# Mean Absolute Error
MAE_train_lr = metrics.mean_absolute_error(Y_train, training_data_prediction_lr)
print("Mean Absolute Error = ", MAE_train_lr)

# Root Mean Squared Error
MSE_train_lr = metrics.mean_squared_error(Y_train, training_data_prediction_lr)
print("Root Mean Squared Error = ", math.sqrt(MSE_train_lr))

# Evaluate linear regression model on test set

test_data_prediction_lr = lr.predict(X_test)

# Linear regression model performance on test set

# R squared error
r2_test_lr = metrics.r2_score(Y_test, test_data_prediction_lr)
print('R Squared value = ', r2_test_lr)

# Mean Absolute Error
MAE_test_lr = metrics.mean_absolute_error(Y_test, test_data_prediction_lr)
print("Mean Absolute Error = ", MAE_test_lr)

# Root Mean Squared Error
MSE_test_lr = metrics.mean_squared_error(Y_test, test_data_prediction_lr)
print("Root Mean Squared Error = ", math.sqrt(MSE_test_lr))

"""##XGBoost regressor"""

regressor = XGBRegressor(max_depth=2,
                         n_estimators=100,
                         learning_rate=0.2,
                         min_child_weight = 200,
                         reg_lambda=200,
                         reg_alpha=10,
                         gamma=700)

# Train the model on training set

regressor.fit(X_train, Y_train)

# prediction on training data
training_data_prediction_XGB = regressor.predict(X_train)

# XGBoost regression model performance on training set

# R squared Value
r2_train_XGB = metrics.r2_score(Y_train, training_data_prediction_XGB)
print('R Squared value = ', r2_train_XGB)

# Mean Absolute Error
MAE_train_XGB = metrics.mean_absolute_error(Y_train, training_data_prediction_XGB)
print("Mean Absolute Error = ", MAE_train_XGB)

# Root Mean Squared Error
MSE_train_XGB = metrics.mean_squared_error(Y_train, training_data_prediction_XGB)
print("Root Mean Squared Error = ", math.sqrt(MSE_train_XGB))

# prediction on test data
test_data_prediction_XGB = regressor.predict(X_test)

# XGBoost regression model performance on test set

# R squared Value
r2_test_XGB = metrics.r2_score(Y_test, test_data_prediction_XGB)
print('R Squared value = ', r2_test_XGB)

# Mean Absolute Error
MAE_test_XGB = metrics.mean_absolute_error(Y_test, test_data_prediction_XGB)
print("Mean Absolute Error = ", MAE_test_XGB)

# Root Mean Squared Error
MSE_test_XGB = metrics.mean_squared_error(Y_test, test_data_prediction_XGB)
print("Root Mean Squared Eror = ", math.sqrt(MSE_test_XGB))

# Hyperparameter tuning the XGBoost model using grid search

param_grid = {
    'learning_rate': [0.01, 0.1, 1],
    'max_depth': [3, 5, 7],
    'n_estimators': [50, 100, 200],
    'reg_lambda': [100, 150, 200],
    'reg_alpha': [5, 10, 20]
}
grid_search = GridSearchCV(estimator=regressor, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, Y_train)

print("Best hyperparameters: ", grid_search.best_params_)
print("Best score: ", grid_search.best_score_)

# Plot the graphs of grid search results

# Extract the hyperparameter values and mean test scores from GridSearchCV.cv_results_
results = grid_search.cv_results_
learning_rates = results['param_learning_rate'].data
max_depths = results['param_max_depth'].data
n_estimators = results['param_n_estimators'].data
mean_test_scores = results['mean_test_score']

# Convert the hyperparameter values to numeric arrays
learning_rates = [float(rate) for rate in learning_rates]
max_depths = [int(depth) for depth in max_depths]
n_estimators = [int(n_estimator) for n_estimator in n_estimators]

# Create a dictionary to store the mean test scores for each hyperparameter value
learning_rate_scores = {}
max_depth_scores = {}
n_estimators_scores = {}
for i, rate in enumerate(learning_rates):
    if rate not in learning_rate_scores:
        learning_rate_scores[rate] = []
    learning_rate_scores[rate].append(mean_test_scores[i])
for i, depth in enumerate(max_depths):
    if depth not in max_depth_scores:
        max_depth_scores[depth] = []
    max_depth_scores[depth].append(mean_test_scores[i])
for i, n_estimator in enumerate(n_estimators):
    if n_estimator not in n_estimators_scores:
        n_estimators_scores[n_estimator] = []
    n_estimators_scores[n_estimator].append(mean_test_scores[i])

# Plot line graphs for each hyperparameter value
fig, axs = plt.subplots(1, 3, figsize=(20, 5))
for rate, scores in learning_rate_scores.items():
    axs[0].plot(range(1, len(scores) + 1), scores, label=f'Learning Rate: {rate:.2f}', linewidth=3)
for depth, scores in max_depth_scores.items():
    axs[1].plot(range(1, len(scores) + 1), scores, label=f'Max Depth: {depth}', linewidth=3)
for n_estimator, scores in n_estimators_scores.items():
    axs[2].plot(range(1, len(scores) + 1), scores, label=f'n_estimators: {n_estimator}', linewidth=3)
axs[0].set_xlabel('Iterations')
axs[0].set_ylabel('Mean Test Score')
axs[0].set_title('Learning Rate')
axs[0].legend()
axs[1].set_xlabel('Iterations')
axs[1].set_ylabel('Mean Test Score')
axs[1].set_title('Max Depth')
axs[1].legend()
axs[2].set_xlabel('Iterations')
axs[2].set_ylabel('Mean Test Score')
axs[2].set_title('n_estimators')
axs[2].legend()
plt.tight_layout()
plt.show()

"""##Random Forest"""

from sklearn.ensemble import RandomForestRegressor

rf= RandomForestRegressor(n_estimators=200,
                          max_depth=8,
                          max_features=11,
                          min_samples_split=100,
                          min_samples_leaf=50,
                          ccp_alpha=0.01)

rf.fit(X_train,Y_train)

# prediction on training data
training_data_prediction_RF = rf.predict(X_train)

# Random Forest regression model performance on training set

# R squared Value
r2_train_RF = metrics.r2_score(Y_train, training_data_prediction_RF)
print('R Squared value = ', r2_train_RF)

# Mean Absolute Error
MAE_train_RF = metrics.mean_absolute_error(Y_train, training_data_prediction_RF)
print("Mean Absolute Error = ", MAE_train_RF)

# Root Mean Squared Error
MSE_train_RF = metrics.mean_squared_error(Y_train, training_data_prediction_RF)
print("Root Mean Squared Error = ", math.sqrt(MSE_train_RF))

# prediction on test data
test_data_prediction_RF = rf.predict(X_test)

# Random Forest regression model performance on test set

# R squared Value
r2_test_RF = metrics.r2_score(Y_test, test_data_prediction_RF)
print('R Squared value = ', r2_test_RF)

# Mean Absolute Error
MAE_test_RF = metrics.mean_absolute_error(Y_test, test_data_prediction_RF)
print("Mean Absolute Error = ", MAE_test_RF)

# Root Mean Squared Error
MSE_test_RF = metrics.mean_squared_error(Y_test, test_data_prediction_RF)
print("Root Mean Squared Eror = ", math.sqrt(MSE_test_RF))

# Hyperparameter tuning the Random Forest model using grid search

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'ccp_alpha': [0.001, 0.01, 0.1]
}
grid_search = GridSearchCV(rf, param_grid, cv=5)
grid_search.fit(X_train, Y_train)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best hyperparameters: ", grid_search.best_params_)
print("Best score: ", grid_search.best_score_)

# Extract the hyperparameter values and mean test scores from GridSearchCV.cv_results_
results = grid_search.cv_results_
min_samples_split = results['param_min_samples_split'].data
max_depths = results['param_max_depth'].data
ccp_alphas = results['param_ccp_alpha'].data
mean_test_scores = results['mean_test_score']

# Convert the hyperparameter values to numeric arrays
min_samples_split = [float(rate) for rate in min_samples_split]
max_depths = [int(depth) for depth in max_depths]
ccp_alphas = [float(ccp_alpha) for ccp_alpha in ccp_alphas]

# Create a dictionary to store the mean test scores for each hyperparameter value
min_samples_split_scores = {}
max_depth_scores = {}
ccp_alpha_scores = {}
for i, rate in enumerate(min_samples_split):
    if rate not in min_samples_split_scores:
        min_samples_split_scores[rate] = []
    min_samples_split_scores[rate].append(mean_test_scores[i])
for i, depth in enumerate(max_depths):
    if depth not in max_depth_scores:
        max_depth_scores[depth] = []
    max_depth_scores[depth].append(mean_test_scores[i])
for i, ccp_alpha in enumerate(ccp_alphas):
    if ccp_alpha not in ccp_alpha_scores:
        ccp_alpha_scores[ccp_alpha] = []
    ccp_alpha_scores[ccp_alpha].append(mean_test_scores[i])

# Plot line graphs for each hyperparameter value
fig, axs = plt.subplots(3, 1, figsize=(10, 18))
for rate, scores in min_samples_split_scores.items():
    axs[0].plot(range(1, len(scores) + 1), scores, label=f'min_samples_split: {rate:.2f}', linewidth=3)
for depth, scores in max_depth_scores.items():
    axs[1].plot(range(1, len(scores) + 1), scores, label=f'Max Depth: {depth}', linewidth=3)
for ccp_alpha, scores in ccp_alpha_scores.items():
    axs[2].plot(range(1, len(scores) + 1), scores, label=f'ccp_alpha: {ccp_alpha}', linewidth=3)
axs[0].set_xlabel('Iterations')
axs[0].set_ylabel('Mean Test Score')
axs[0].set_title('min_samples_split')
axs[0].legend()
axs[1].set_xlabel('Iterations')
axs[1].set_ylabel('Mean Test Score')
axs[1].set_title('Max Depth')
axs[1].legend()
axs[2].set_xlabel('Iterations')
axs[2].set_ylabel('Mean Test Score')
axs[2].set_title('ccp_alpha')
axs[2].legend()
plt.tight_layout()
plt.show()

